# -*- coding: utf-8 -*-
"""MLE_Intern_Task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_LRqVl33w9w81FfgDG_EgE9ZaBxeGrsd

# **IMPORTING LIBRARIES**
"""

import os
import gc
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import backend as K

"""# **IMPORTING THE SENTIMENT-140 DATASET**"""

from google.colab import drive
drive.mount("/content/drive")

# Specify the encoding as 'latin-1' to handle characters that are not valid in utf-8
# Added engine='python' to handle potential parsing issues with delimiters or quoting
# Explicitly setting sep and quotechar based on standard CSV format
data = pd.read_csv("/content/training.1600000.processed.noemoticon.csv",
                   encoding='latin-1',
                   engine='python',
                   sep=',',          # Explicitly set the separator to comma
                   quotechar='"' ,
                   on_bad_lines='skip'# Explicitly set the quote character to double quote
                  )

"""# **DATA PRE PROCESSING**

### Renamed default column names to meaningful labels. Mapped sentiment labels from numeric (0, 2, 4) to textual categories (negative, neutral, positive) for easier interpretation.
"""

data.columns = ['sentiment', 'tweet_id', 'date', 'query', 'user', 'text']
sentiment_map = {0: 'negative', 2: 'neutral', 4: 'positive'}
data['sentiment'] = data['sentiment'].map(sentiment_map)

"""# **CLEANING THE TWEETS**
### Cleaned each tweet by removing URLs, mentions, hashtags, non-alphabetic characters, and stopwords. Applied lemmatization using NLTK to normalize the words. This prepares the data for feature extraction.
"""

nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
    text = re.sub(r'\@\w+|\#','', text)
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

data['clean_text'] = data['text'].apply(clean_text)

"""# **EXPLORATORY DATA ANALYSIS (EDA)**
### Performed basic EDA to understand the class distribution of sentiments and analyze the length of cleaned tweets. Also identified the most common words for each sentiment category.
"""

sns.countplot(data['sentiment'])
plt.title('Sentiment Distribution')
plt.show()

data['text_length'] = data['clean_text'].apply(lambda x: len(x.split()))
sns.histplot(data['text_length'], bins=30, kde=True)
plt.title('Tweet Length Distribution')
plt.show()

"""No common words in the neutral case, so removed the consequent part for the neutral ones."""

from wordcloud import WordCloud
for sentiment in ['negative', 'positive']:
    text = ' '.join(data[data['sentiment']==sentiment]['clean_text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 4))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"WordCloud for {sentiment} tweets")
    plt.show()

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english')
X_ngrams = vectorizer.fit_transform(data['clean_text'])
top_ngrams = zip(vectorizer.get_feature_names_out(), X_ngrams.sum(axis=0).tolist()[0])
top_ngrams = sorted(top_ngrams, key=lambda x: x[1], reverse=True)[:10]
print("Top 10 Bigrams:", top_ngrams)

"""# **TRAINING AND TESTING PART OF THE NOTEBOOK**
We are going to test different models on our dataset. There are multiple models avalaible and we are going to test it on Logistic Regression, Multinomial Naive Bayes, Random Forest Classifier, a Deep Neural Network an a pre-trained BERT model.
"""

for sentiment in ['negative', 'neutral', 'positive']:
    words = ' '.join(data[data['sentiment']==sentiment]['clean_text']).split()
    common_words = Counter(words).most_common(10)
    print(f"Most common words in {sentiment} tweets:", common_words)

X = data['clean_text']
y = data['sentiment']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""### Used TfidfVectorizer to convert cleaned text into numeric feature vectors. Limited to the top 5000 features to reduce dimensionality while retaining meaningful context."""

vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

"""Trained two classical machine learning models: Logistic Regression and Multinomial Naive Bayes. These serve as strong baselines for text classification.

# **Logistic Regression**
"""

lr = LogisticRegression(max_iter=200)
lr.fit(X_train_vec, y_train)

"""# **Multinomial Naive Bayes**"""

nb = MultinomialNB()
nb.fit(X_train_vec, y_train)

"""# **Random Forest Classifier**

#### Trained a Random Forest model using lightweight settings to handle the large dataset quickly. Optimizations include shallow trees and fewer estimators to trade off minimal accuracy for faster computation. While runnning without any sped-up computation, the model was taking an abruptly large amount of time, even over an hour. Time is a necessary component of our work so had to deploy this.
"""

# We only need to calculate the predictions for the fast Random Forest here if it hasn't been done yet.
# Check if the fast Random Forest model has been trained and make predictions if needed.
try:
    rf_fast_pred  # Check if rf_fast_pred exists (meaning the fast RF was trained)
except NameError:
    print("Fast Random Forest model not trained. Training now...")
    from sklearn.ensemble import RandomForestClassifier
    import time

    # For very large datasets - prioritize speed over slight accuracy loss
    rf_fast = RandomForestClassifier(
        n_estimators=25,           # Very few trees
        max_depth=8,               # Shallow trees
        min_samples_split=50,      # Large minimum split
        min_samples_leaf=25,       # Large minimum leaf
        max_features='log2',       # Even fewer features per split
        max_samples=0.6,           # Use only 60% of data per tree
        n_jobs=-1,
        random_state=42,
        verbose=1
    )

    start_time = time.time()
    rf_fast.fit(X_train_vec, y_train)
    fast_time = time.time() - start_time

    rf_fast_pred = rf_fast.predict(X_test_vec)
    print(f"Ultra-fast training time: {fast_time:.2f} seconds ({fast_time/60:.2f} minutes)")
    from sklearn.metrics import accuracy_score
    print("Fast RF Accuracy:", accuracy_score(y_test, rf_fast_pred))

"""# **A DEEP NEURAL NETWORK AT WORK**

### üîß Deep Neural Network (DNN) ‚Äì Memory Optimized Training

In this section, we build and train a Deep Neural Network (DNN) to classify tweet sentiment as **positive**, **neutral**, or **negative**.

‚öôÔ∏è To handle the **large Sentiment140 dataset (1.6M tweets)** without running into memory issues, the code uses:
- A **custom batch-wise generator** to convert sparse vectors to dense arrays on-the-fly
- The `tf.data.Dataset` API for efficient input pipelining
- Dropout layers for regularization
- GPU memory growth configuration
- Early stopping and model checkpointing for best performance

üß† The model architecture is simple but effective:
- Dense ‚Üí Dropout ‚Üí Dense ‚Üí Dropout ‚Üí Output

üèÅ After training, we evaluate the model on test data using:
- Accuracy score
- Classification report (precision, recall, F1-score)

This version runs significantly faster and safer on large datasets than naive implementations.
"""

print("=== DEEP NEURAL NETWORK - MEMORY OPTIMIZED ===")

# Enable memory growth for GPU
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# Clear session and garbage once before training
K.clear_session()
gc.collect()

# Label encoding and one-hot
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)
y_train_cat = to_categorical(y_train_encoded)
y_test_cat = to_categorical(y_test_encoded)

# üîÅ Batch-wise generator to avoid .toarray() calls in loop
def sparse_batch_generator(X_sparse, y, batch_size=64, shuffle=True):
    indices = np.arange(X_sparse.shape[0])
    if shuffle:
        np.random.shuffle(indices)
    for start in range(0, len(indices), batch_size):
        end = min(start + batch_size, len(indices))
        batch_idx = indices[start:end]
        X_batch = np.array([X_sparse[i].toarray().flatten() for i in batch_idx], dtype=np.float32)
        y_batch = y[batch_idx].astype(np.float32)
        yield X_batch, y_batch

# ‚úÖ Memory-efficient tf.data.Dataset
def get_tf_dataset(X_sparse, y, batch_size=64, shuffle=True):
    output_signature = (
        tf.TensorSpec(shape=(None, X_sparse.shape[1]), dtype=tf.float32),
        tf.TensorSpec(shape=(None, y.shape[1]), dtype=tf.float32)
    )
    dataset = tf.data.Dataset.from_generator(
        lambda: sparse_batch_generator(X_sparse, y, batch_size, shuffle),
        output_signature=output_signature
    )
    return dataset.prefetch(tf.data.AUTOTUNE)

# Set batch size
BATCH_SIZE = 64

# Prepare datasets
train_size = int(len(y_train_cat) * 0.8)
train_dataset = get_tf_dataset(X_train_vec[:train_size], y_train_cat[:train_size], BATCH_SIZE, shuffle=True)
val_dataset = get_tf_dataset(X_train_vec[train_size:], y_train_cat[train_size:], BATCH_SIZE, shuffle=False)
test_dataset = get_tf_dataset(X_test_vec, y_test_cat, BATCH_SIZE, shuffle=False)

# ‚úÖ Build compact DNN model
dnn_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_vec.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(len(np.unique(y_train)), activation='softmax')  # No float16
])

# Compile
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
dnn_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

print("Model compiled successfully")
print(f"Model parameters: {dnn_model.count_params():,}")

# Callbacks
checkpoint = ModelCheckpoint('dnn_model_best.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True, verbose=1)

# ‚úÖ Train
try:
    history = dnn_model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=5,
        callbacks=[checkpoint, early_stop],
        verbose=1
    )
    print("Training completed successfully!")

    # Load best weights
    dnn_model.load_weights('dnn_model_best.h5')

    # Predict in batches (no session clearing inside)
    print("Making predictions...")
    predictions_list = []
    for x_batch, _ in test_dataset:
        batch_pred = dnn_model.predict(x_batch, verbose=0)
        predictions_list.append(batch_pred)

    dnn_pred_proba = np.vstack(predictions_list)
    dnn_pred_encoded = np.argmax(dnn_pred_proba, axis=1)
    dnn_pred = label_encoder.inverse_transform(dnn_pred_encoded)

    # Evaluation
    from sklearn.metrics import accuracy_score, classification_report
    print("Accuracy:", accuracy_score(y_test, dnn_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, dnn_pred))

except Exception as e:
    print(f"Training failed: {e}")
    print("Try reducing BATCH_SIZE further or simplifying the model.")

finally:
    K.clear_session()
    gc.collect()
    print("Memory cleanup completed")

"""# **PREDICTION and COMPARING ALL THE MODELS**"""

# Step 1: Existing predictions
y_pred_lr = lr.predict(X_test_vec)
y_pred_nb = nb.predict(X_test_vec)
y_pred_rf = rf_fast_pred  # Already predicted earlier


# Step 2
# Efficient batch-wise DNN prediction
def sparse_batch_generator(X_sparse, batch_size=64):
    for start in range(0, X_sparse.shape[0], batch_size):
        end = min(start + batch_size, X_sparse.shape[0])
        batch = np.array([X_sparse[i].toarray().flatten() for i in range(start, end)], dtype=np.float32)
        yield batch

dnn_pred_batches = []
for x_batch in sparse_batch_generator(X_test_vec, batch_size=64):
    batch_output = dnn_model.predict(x_batch, verbose=0)
    dnn_pred_batches.append(batch_output)

dnn_output = np.vstack(dnn_pred_batches)
dnn_pred = label_encoder.inverse_transform(np.argmax(dnn_output, axis=1))


# Step 3: BERT predictions (already predicted earlier)
# bert_pred_labels = label_encoder_bert.inverse_transform(bert_preds.numpy())

# Step 3: Evaluation
models = ['Logistic Regression', 'Fast Random Forest', 'Naive Bayes', 'Deep Neural Network']
predictions = [y_pred_lr, y_pred_rf, y_pred_nb, dnn_pred]

accuracies = []

print("\n=== MODEL ACCURACY COMPARISON ===")
for i, pred in enumerate(predictions):
    acc = accuracy_score(y_test, pred)
    accuracies.append(acc)
    print(f"{models[i]} Accuracy: {acc:.4f}")

"""## **A PLOT COMPARING ALL THE MODELS USED**"""

plt.figure(figsize=(12, 6))
colors = ['blue', 'green', 'red', 'purple']
plt.bar(models, accuracies, color=colors)
plt.title('Model Accuracy Comparison')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""# **CONFUSION MATRIX FOR BEST PERFORMING MODEL**"""

# Get best model index
best_model_idx = np.argmax(accuracies)
best_model = models[best_model_idx]
best_pred = predictions[best_model_idx]

# Plot confusion matrix for best model
plt.figure(figsize=(8, 6))
class_labels = sorted(list(set(y_test)))  # Ensure correct class order
cm = confusion_matrix(y_test, best_pred, labels=class_labels)
sns.heatmap(cm, annot=True, fmt='d',
            xticklabels=class_labels,
            yticklabels=class_labels,
            cmap='Blues')
plt.title(f'Confusion Matrix - {best_model}')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()

"""# **A SUMMARY TABLE FOR THE RESULTS**"""

# Create summary table
results_df = pd.DataFrame({
    'Model': models,
    'Accuracy': [f"{acc:.4f}" for acc in accuracies]
})

"""# **A SUMMARY FOR ALL THE MODELS USED:**"""

# Ensure all predictions are available
y_pred_lr = lr.predict(X_test_vec)
y_pred_nb = nb.predict(X_test_vec)
y_pred_rf = rf_fast_pred

# DNN

def sparse_batch_generator(X_sparse, batch_size=64):
    n = X_sparse.shape[0]
    for start in range(0, n, batch_size):
        end = min(start + batch_size, n)
        dense_batch = np.asarray(
            [X_sparse[i].toarray().ravel() for i in range(start, end)],
            dtype=np.float32
        )
        yield dense_batch

def sparse_batch_generator(X_sparse, batch_size=64):
    n = X_sparse.shape[0]
    for start in range(0, n, batch_size):
        end = min(start + batch_size, n)
        dense_batch = np.asarray(
            [X_sparse[i].toarray().ravel() for i in range(start, end)],
            dtype=np.float32
        )
        yield dense_batch

# DNN predictions in memory-safe batches
dnn_probs = []
for xb in sparse_batch_generator(X_test_vec, batch_size=64):
    dnn_probs.append(dnn_model.predict(xb, verbose=0))

dnn_output = np.vstack(dnn_probs)
dnn_pred   = label_encoder.inverse_transform(dnn_output.argmax(axis=1))

# === Classification Reports ===
print("\n=== MODEL PERFORMANCE SUMMARY ===")

print("\nüìå Logistic Regression:")
print(classification_report(y_test, y_pred_lr))

print("\nüìå Naive Bayes:")
print(classification_report(y_test, y_pred_nb))

print("\nüìå Fast Random Forest:")
print(classification_report(y_test, y_pred_rf))

print("\nüìå Deep Neural Network:")
print(classification_report(y_test, dnn_pred))

"""Generated confusion matrices and classification reports to analyze the precision, recall, and F1-score of each model. Used seaborn to visually inspect misclassifications.

# **PREDICTION OF THE SENTIMENT FOR A NEW TWEET USING ALL THE TRAINED MODELS**
"""

def predict_sentiment(tweet):
    print(f"\nüìù Input Tweet: {tweet}")

    # Clean same as training
    def clean_text(text):
        text = text.lower()
        text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
        text = re.sub(r'\@\w+|\#','', text)
        text = re.sub(r'[^a-z\s]', '', text)
        tokens = text.split()
        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
        return ' '.join(tokens)

    cleaned = clean_text(tweet)
    vec = vectorizer.transform([cleaned])

    print("\nüîç Model Predictions:")

    # Logistic Regression
    print("Logistic Regression:", lr.predict(vec)[0])

    # Naive Bayes
    print("Naive Bayes:", nb.predict(vec)[0])

    # Random Forest
    print("Random Forest:", rf_fast.predict(vec)[0])

    # DNN
    dnn_input = tf.convert_to_tensor(vec.toarray().astype(np.float32))
    dnn_output = dnn_model.predict(dnn_input)
    dnn_pred = label_encoder.inverse_transform([np.argmax(dnn_output)])
    print("DNN:", dnn_pred[0])

    # # BERT
    # bert_tokens = tokenizer(tweet, return_tensors='tf', truncation=True, padding=True)
    # bert_logits = bert_model.predict(bert_tokens).logits
    # bert_pred = tf.argmax(bert_logits, axis=1).numpy()[0]
    # print("BERT:", label_encoder_bert.inverse_transform([bert_pred])[0])

"""# **RESULTS FOR THREE DIFFERENT SENTENCES**"""

predict_sentiment("I absolutely love this new phone! It's amazing.üî•")
predict_sentiment("The update ruined everything. I'm so disappointed.")
predict_sentiment("It's okay, not good, not bad.")